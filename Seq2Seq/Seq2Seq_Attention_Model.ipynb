{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Seq2Seq with Attention Mechanism (model only)***\n",
        "\n",
        "Here, in this notebook, we will implement a Seq2Seq model with attention mechanism. There are mainly two types of alignment mechanism for attention namely Luong and Badhanau Attention. We will be using Badhanau Attention in this case.\n",
        "\n",
        "\n",
        "Bahdanau attention can learn more complex relations between the data than other types of attention mechanisms because it employs a neural network to compute the attention weights rather than a simple mathematical algorithm.\n",
        "\n",
        "\n",
        "You can find more information about these attention types [here](https://https://www.baeldung.com/cs/attention-luong-vs-bahdanau)."
      ],
      "metadata": {
        "id": "YzaBSHyGbtSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random"
      ],
      "metadata": {
        "id": "hURG18bEdfS2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder**"
      ],
      "metadata": {
        "id": "MU0xAl2fdTI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Yw_2WU8Sbb44"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self,input_size, embed_size, hidden_size, num_layers, dropout):\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embed_size)\n",
        "    self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, bidirectional = True)\n",
        "\n",
        "    self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    encoder_states, (hidden, cell) = self.rnn(embedding)\n",
        "\n",
        "    hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]),dim=2))\n",
        "\n",
        "    cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "    return encoder_states, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaining the Encoder block.**"
      ],
      "metadata": {
        "id": "ELySs0RWhY6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2Seq is for language related task that carries out operation sequentially using Encoder Decoder RNN architectures.\n",
        "\n",
        "\n",
        "\n",
        "We will need following elements for making an encoder block.\n",
        "*   input_size = size of source vocab\n",
        "*   embed_size = dimension of embedding that you want the words to be represented in\n",
        "* hidden_size = size of hidden layer created by RNN\n",
        "* num_layer = how many layer of RNN that you want\n",
        "\n",
        "We have used a Bidirectional LSTM. When `bidirectional=True`, output will contain a concatenation of the forward and reverse hidden states at each time step in the sequence.\n",
        "\n",
        "So, the encoder_state, hidden, and cell will have size of `2*hidden_size`.\n",
        "\n",
        "As we will be forming a context vector that has information of each LSTM cell. The cell and hidden values of each LSTM cell will carry that informaition forward to context_vector. So, instead of using all or just one of those hidden information by choosing by ourselves, we will let the NN decide by itself.\n",
        "\n",
        "`fc_hidden` and `fc_cell` will convert size of `2*hidden_size` into size of `hidden_size` via the use of Linear Neural Network.\n"
      ],
      "metadata": {
        "id": "78qU6GnohjBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Block**\n",
        "\n",
        "\n",
        "* Initially, `X` has dimension of `(sequence_length, batch_size)`.\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "* `embedding = self.dropout(self.embedding(x))` will create embedding of X with dropout. It has extra dimension as we have created embeddings of each word for Machine to understand it well. The more dimension, the better but it will require higher computation resources.\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "* `encoder_states, (hidden, cell) = self.rnn(embedding)`. LSTM will output in encoder_state, (hidden,cell) where encoder_state has dimension `(seq_length, N, hidden_size)` where N is batch_size. But as we have used `bidirectional=True`, the dimension will be `(seq_length, N, hidden_size * 2)`. [For more info on LSTM](https://https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "* hidden and cell have dimension of `(num_layer, N, hidden_size*2)` or let's say, `(1,N,hidden_size*2)`\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "* `hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))`, we have concatenated forward and backward hidden into one. Then we sent it through `fc_hidden` for NN to make it into size of `hidden_size` by choosing itself what information it finds important.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "* We have done similar for `cell` using `fc_cell`.\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "* We return `encoder_states, hidden, cell` from the function\n"
      ],
      "metadata": {
        "id": "yot35U-XkmKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "GT66zL3grka1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "J9AdnPCRrmME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decoder**"
      ],
      "metadata": {
        "id": "N4Qh_s-trnGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, embed_size, hidden_size, output_size, num_layers, dropout):\n",
        "\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embed_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = nn.LSTM(2*hidden_size+embed_size, hidden_size, num_layers)\n",
        "\n",
        "    self.energy = nn.Linear(3*hidden_size, 1)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax(0)\n",
        "\n",
        "\n",
        "  def forward(self, x, encoder_states, hidden, cell):\n",
        "\n",
        "    # Dimensions are as\n",
        "    # x -> (N)  : <SOS> tokens of all instances i.e. of size = Batch size\n",
        "    # encoder_states -> (seq_len, N, 2*hidden_size)\n",
        "    # hidden -> (1, N, hidden_size)\n",
        "    # cell -> (1, N, hidden_size)\n",
        "\n",
        "    x = x.unsqueeze(0)\n",
        "    # x -> (1,N)\n",
        "\n",
        "    embeddings = self.dropout(self.embedding(x))\n",
        "    # embeddings -> (1,N,embed_size)\n",
        "\n",
        "    seq_len = encoder_states.shape[0]\n",
        "    h_reshaped = hidden.repeat(seq_len, 1, 1)\n",
        "    # h_reshaped -> (seq_length, N, hidden_size)\n",
        "    # you can check what it does in your terminal by trying (>> x = torch.randn(1, 2, 5) >> x.repeat(3,1,1) >>x)\n",
        "\n",
        "    # we will concat h_reshaped with encoder_states to compute energy. It was the reason for reshaping the hidden.\n",
        "    energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
        "    # self.energy takes 3*hidden_size as input and gives out a single output. That output is then passed via relu.\n",
        "    # energy -> (seq_len, N, 1)\n",
        "\n",
        "    attention = self.softmax(energy)\n",
        "    # attention -> (seq_len,N,1)\n",
        "    # The softmax operation normalizes the values along the sequence length dimension, ensuring they sum to 1 and represent probabilities.\n",
        "\n",
        "    # attention -> snk -> (seq_len, N, 1)\n",
        "    # encoder_states -> snl -> (seq_len, N, hidden_size*2)\n",
        "    # we want context vector of dimension knl i.e. (1,N,hidden_size*2)\n",
        "    context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
        "    # context_vector -> (1,N,hidden_size*2)\n",
        "\n",
        "\n",
        "    # For decoder rnn, input will be context_vector and embeddings of target, the context vector will be concatenated with embedding\n",
        "    # embeddings -> (1,N,embed_size)\n",
        "    rnn_input = torch.cat((context_vector, embeddings), dim=2)\n",
        "    # rnn_input: (1, N, hidden_size*2 + embed_size)\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "    # outputs shape: (1, N, hidden_size)\n",
        "    # (hidden,cell) are provided to the LSTM (in self.rnn layer) so that it will take previous context info of (hidden,cell) for initialization\n",
        "    #When processing the first time step of a sequence, the RNN or LSTM needs an initial hidden state (hidden) and cell state (cell) to start the sequence processing.\n",
        "    # These states act as memory from previous time steps.\n",
        "\n",
        "    # outputs -> (1,N,hidden_size) ; self.fc = Linear(hidden_size,output_size) -> (1,N,output_size)\n",
        "    predictions = self.fc(outputs).squeeze(0)\n",
        "    # predictions: (N, output_size)\n",
        "\n",
        "    return predictions, hidden, cell\n"
      ],
      "metadata": {
        "id": "XM6w-HtresN7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Seq2Seq**"
      ],
      "metadata": {
        "id": "Jue7_KbkUIHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder,target_vocab_size,device):\n",
        "      super(Seq2Seq, self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "      self.target_vocab_size = target_vocab_size\n",
        "      self.device= device\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "      batch_size = source.shape[1]\n",
        "      target_len = target.shape[0]\n",
        "      target_vocab_size = self.target_vocab_size\n",
        "\n",
        "      outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
        "      # outputs -> (seq_len, N, output_size)\n",
        "\n",
        "      encoder_states, hidden, cell = self.encoder(source)\n",
        "      # encoder_states -> (seq_len, N, hidden_size*2)\n",
        "      # hidden -> (1,N,hidden)\n",
        "      # cell -> (1,N,hidden)\n",
        "\n",
        "      # First input will be <SOS> token\n",
        "      x = target[0] # x -> (N)\n",
        "\n",
        "      for t in range(1, target_len):\n",
        "          # At every time step use encoder_states and update hidden, cell\n",
        "\n",
        "          output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "          # output -> (N, output_size)\n",
        "          # hidden -> (1,N,hidden)\n",
        "          # cell -> (1,N,hidden)\n",
        "\n",
        "\n",
        "          # Store prediction for current time step\n",
        "          outputs[t] = output\n",
        "\n",
        "          # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "          best_guess = output.argmax(1)\n",
        "\n",
        "          # With probability of teacher_force_ratio we take the actual next word\n",
        "          # otherwise we take the word that the Decoder predicted it to be.\n",
        "          # Teacher Forcing is used so that the model gets used to seeing\n",
        "          # similar inputs at training and testing time, if teacher forcing is 1\n",
        "          # then inputs at test time might be completely different than what the\n",
        "          # network is used to. This was a long comment.\n",
        "          x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "      return outputs\n"
      ],
      "metadata": {
        "id": "CMujbt23TlhN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Building**"
      ],
      "metadata": {
        "id": "wZyHUXVeWGtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_net = Encoder(input_size=10000,\n",
        "                      embed_size=256,\n",
        "                      hidden_size=512,\n",
        "                      num_layers=1,\n",
        "                      dropout=0.5\n",
        "                      )"
      ],
      "metadata": {
        "id": "95dchUjkUS1O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_net = Decoder(input_size=10000,\n",
        "                      embed_size=256,\n",
        "                      hidden_size=512,\n",
        "                      output_size=5000,\n",
        "                      num_layers=1,\n",
        "                      dropout=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "7fHoRzSeWqVL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "P4oOHwomXL4e",
        "outputId": "abd6000f-4dc7-424b-94bc-5be4c80c058b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq(encoder=encoder_net,\n",
        "                decoder=decoder_net,\n",
        "                target_vocab_size=5000,\n",
        "                device=device\n",
        "                )"
      ],
      "metadata": {
        "id": "oj00S4uDW8xu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTEJswyiXWfj",
        "outputId": "80447183-2190-46f9-ff1e-f91414085866"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(10000, 256)\n",
              "    (rnn): LSTM(256, 512, bidirectional=True)\n",
              "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10000, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(1280, 512)\n",
              "    (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
              "    (fc): Linear(in_features=512, out_features=5000, bias=True)\n",
              "    (relu): ReLU()\n",
              "    (softmax): Softmax(dim=0)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDKxQeiKXsBw",
        "outputId": "148b9a67-c5d8-481c-aaad-cbd0bdaf1a0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 15,564,169 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is it for Seq2Seq model with attention. Thank you😀"
      ],
      "metadata": {
        "id": "_2b6vOBcYFLo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6uQYEbJvYSLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
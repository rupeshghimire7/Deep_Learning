{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_Z-N7Ylp0-Gi",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ce13b5ea561fdc64db96b3016ced66",
     "grade": false,
     "grade_id": "cell-5c7c35e5db56651f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Machine Translation using Sequence to Sequence LSTM networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7BEzkQrJ0-Gn",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c09f58dd19c22fd8c14a82ecfdb3ad36",
     "grade": false,
     "grade_id": "cell-86a6d09c34d33f25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Machine translation refers to the use of machines or software to translate text to speech or speech from one language to another language.\n",
    "\n",
    "In this assignment we will work out the demonstration of how we can apply the LSTM networks to translate speech from one language to another.\n",
    "\n",
    "We will be translating sequences from English to Chinese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pGuEgkXT0-Gn",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aaf14ad60b6057ae5ef2f283fae3a801",
     "grade": false,
     "grade_id": "cell-e29498e719ab95c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Datasets\n",
    "We will be using this dataset. https://www.manythings.org/anki/cmn-eng.zip.\n",
    "\n",
    "Few samples of the dataset looks as follows:\n",
    "\n",
    "**$ENGLISH \\hspace{10mm} CHINESE$**\n",
    "\n",
    "$Go. \\hspace{30mm} 走 $\n",
    "\n",
    "$Run! \\hspace{30mm} 跑!$\n",
    "\n",
    "$Fire! \\hspace{30mm} A火！$\n",
    "\n",
    "$Help! \\hspace{30mm} 救命!$\n",
    "\n",
    "$Jump. \\hspace{30mm} 跳.$\n",
    "\n",
    "$Stop! \\hspace{30mm} 停止!$\n",
    "\n",
    "We can see that on the left column, we have a list of english sequences like, GO, RUN, FIRE, HELP, etc and on the right we have their respective CHINESE tranlsations.\n",
    "\n",
    "Here, the input to the model will be the list of English sentences and the target will be the list of Chinese translations.\n",
    "\n",
    "Here in the dataset,\n",
    "\n",
    "We will follow the following steps duing the machine translation:\n",
    "\n",
    "1. Preprocess the training sequenes\n",
    "2. Develop sequence to sequence LSTM model\n",
    "3. Train LSTM model\n",
    "4. Evalute model by testing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AxGshOMk6_n9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fc6f0c2167c4587dc68f0dcbeeac76e",
     "grade": false,
     "grade_id": "cell-322635f771076732",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZCo5hAFPw6MR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "453a2c02cdbbb02cb0046e352998d837",
     "grade": false,
     "grade_id": "cell-92d78258255f0c3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UrdGrSJoxC3o",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe69062cf677017d2c47b789df0f9a32",
     "grade": false,
     "grade_id": "cell-64ea11783dd9398f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vU1L7fAckbxe",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca7a41a47f2a4f4c921145cb562375bc",
     "grade": false,
     "grade_id": "cell-949aff3298d2f0f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's read the dataset from the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mQgGzF147E-j",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56425b9d799dd5ce1268301ab7c58c7e",
     "grade": false,
     "grade_id": "cell-f81b84edf1b80a87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9VCyVeGew6MT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcc930cecb546092836f694d5b4ce982",
     "grade": false,
     "grade_id": "cell-3cdcf87702aba82a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# lines = pd.read_table('/content/drive/My Drive/NMT_Assignment_Fuse/cmn.txt', names=['eng', 'chin', 'info'])\n",
    "lines = pd.read_table('./cmn.txt', names=['eng', 'chin', 'info'])\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dvqLeT1hkS_1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4602bba070f00465a7f411ce333ea381",
     "grade": false,
     "grade_id": "cell-109bf8e6353d477b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We are concerned with only english to chinese translation, so we choose these two columns.\n",
    "### Exercise 1\n",
    "#### Task 1\n",
    "<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n",
    "* Select the English and Chinese translation columns\n",
    "* Select the first 10,000 samples and store it on `lines` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "txXegvQykUFr",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11a7c082ad344bbf53fbc79a134ca6a5",
     "grade": false,
     "grade_id": "cell-8d554e7a8fae2b5f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-1-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-1-Task-1\n",
    "# lines = None\n",
    "\n",
    "# Select `eng` and `chin` columns from the table\n",
    "# take only first 10,000 samples to train\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zgYFaXI_0-Gr",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3d1a3f4c8d47e319282102289d91b29",
     "grade": true,
     "grade_id": "cell-cc75ee800e422bec",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-1-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "assert lines is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Kh7IpJH-w6MT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd3454c7013d5e967f924cdf40bec69",
     "grade": false,
     "grade_id": "cell-c1a557d4e1f790be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# displaying shape of the training set\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qJqfTx_S7MBm",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a3fefeb3dbff03a8151d1ceafe32a43",
     "grade": false,
     "grade_id": "cell-6f124aca6532ebf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Data-Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1p_OI3gCm1nY",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69ae6492cb816a54dd76e1158feb7ff8",
     "grade": false,
     "grade_id": "cell-12b91bc6ebf516bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will follow the following preprocessing steps in order to clean the dataset and fit into model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9OavBlnl0-Gs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0e3ec6a0dcb40c939ed480e66929e06",
     "grade": false,
     "grade_id": "cell-55d8579a96347262",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ayVesOcK0-Gs",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a573be3cc12561afab4c01b19ce01795",
     "grade": false,
     "grade_id": "cell-818a7628e07222c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# lowercase inputs on the columns \n",
    "lines.eng = lines.eng.apply(lambda x: x.lower())\n",
    "lines.chin = lines.chin.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ePdmVLLlw6MU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82f8bf7c2439c84eb248d1b09d073edf",
     "grade": false,
     "grade_id": "cell-ae96a6b81a1185d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Above, we just performed lowercasing of the input samples. Lowercasing is the first operation we performed. Now, we will perform other operations like: `Removing Quotes`, `Removing Special Characters`, `Removing Uneven Spaces`, `Adding <START> and <END> tokens`, etc. Perform similar implementations like that mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d-vf6AbR0-Gs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "068af81cad8ab07cbbceb9a3cbccd94d",
     "grade": false,
     "grade_id": "cell-88c2f1ffb428de99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Removing Quotes\n",
    "<b><div style=\"text-align: right\">[POINTS: 3]</div></b>\n",
    "#### Task 1\n",
    "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "cnxN3CNhw6MU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24f6e9de968fb417c8bddc9654cbe58b",
     "grade": false,
     "grade_id": "cell-166a4b682ff40791",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-2-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-2-Task-1\n",
    "\n",
    "# remove all the quotes \"'\" from the columns\n",
    "\n",
    "# lines.eng = None\n",
    "# lines.chin = None\n",
    "\n",
    "# Exercise 2 | Task 1\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EFKIDmSg0-Gt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8533a681c6890484e1bd4547c9a4380a",
     "grade": true,
     "grade_id": "cell-71fafecfa7e89a02",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-2-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "assert lines.eng is not None\n",
    "assert lines.chin is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dQPsCJ-t0-Gt",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe5f76d98ab6f1851b1222a619d68c5c",
     "grade": false,
     "grade_id": "cell-17278651b7e58ab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3c7SFZZJw6MU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1500be7e854dc4d11e9ab8ce40c44f4a",
     "grade": false,
     "grade_id": "cell-57bd2575631fb4c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set of all special characters\n",
    "sets_of_punctuations = set(string.punctuation)\n",
    "\n",
    "# Removing sets of all special characters from the inputs\n",
    "lines.eng = lines.eng.apply(lambda x: ''.join(char for char in x if char not in sets_of_punctuations))\n",
    "lines.chin = lines.chin.apply(lambda x: ''.join(char for char in x if char not in sets_of_punctuations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zdfLmsrY0-Gt",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e6b1f4a051d2cb789b504fb6beb0685",
     "grade": false,
     "grade_id": "cell-5ae1b11f41d71b27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Removing Uneven Spaces\n",
    "#### Task 2\n",
    "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0NJg9Wtxw6MV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6262580c52c9c0d41e3f48056d849424",
     "grade": false,
     "grade_id": "cell-b5bd2c20a73d1679",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-2-Task-2"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-2-Task-2\n",
    "# lines.eng = None\n",
    "# lines.chin = None\n",
    "\n",
    "# There may be uneven spaces in the inputs\n",
    "# We have to remove the extra spaces too\n",
    "\n",
    "# Exercise 2 | Task 2\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "M4dMDkYn0-Gu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9274c3da54ce529a0a48fd322b407e24",
     "grade": true,
     "grade_id": "cell-557c50952c53e7ed",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-2-Task-2"
    ]
   },
   "outputs": [],
   "source": [
    "assert lines.eng is not None\n",
    "assert lines.chin is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nf_nMZp00-Gu",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50e043f4e44d400cb3a775b965b6b665",
     "grade": false,
     "grade_id": "cell-03607f2602068f04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Adding `<START>` and `<END>` Tokens\n",
    "E.g.\n",
    "'Hi' = '`<START>` Hi `<END>`'\n",
    "#### Task 3\n",
    "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>\n",
    "We will perform this operations over Chinese column samples only because we are converting English sequences to chinese only for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "24kKSFCjw6MW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8df2bf22b3b379332eeeebc57bd287e",
     "grade": false,
     "grade_id": "cell-ad1abc3f8ffea62a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-2-Task-3"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-2-Task-3\n",
    "# lines.chin = None\n",
    "\n",
    "# Adding <START> and <END> tokens with trailing spaces\n",
    "\n",
    "# Exercise 2 | Task 3\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PJA33kQZ0-Gu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a0a377fd98f6659655db1b424338212",
     "grade": true,
     "grade_id": "cell-fd254802897335ea",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-2-Task-3"
    ]
   },
   "outputs": [],
   "source": [
    "assert lines.chin is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ojnbyezHqUaG",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86a9ff8785d8ef6fb2437aae08cf171b",
     "grade": false,
     "grade_id": "cell-d4bdec9ded84970b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, our next task is to create a list of vocabularies of English and Chinese Inputs.\n",
    "\n",
    "Following code will tokenize the words present in the English and Chinese dataset that we use to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vBibz4Gn666N",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8153db3e15c212af999f9d8622e21efc",
     "grade": false,
     "grade_id": "cell-fd22ab95a6103221",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5GtJWhtG7UiM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31a8e3c3129bf64cf3af93c2d1e27ca2",
     "grade": false,
     "grade_id": "cell-3c2d3a9eb7586ce1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Tokenizing the English and the Chinese words in to set `all_english_vocabs` and `all_chinese_vocabs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dY-j6xPpqdPT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb09fd0d0113e567e720dff6a2d57257",
     "grade": false,
     "grade_id": "cell-cdb536778da0a35b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Collect English Vocabs\n",
    "all_english_vocabs = set()\n",
    "for english in lines.eng:\n",
    "    words = english.split()\n",
    "    for word in words:\n",
    "        if word not in all_english_vocabs:\n",
    "            all_english_vocabs.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UJGtunRdtPdX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c81f85aa3a4cd2c1d99800f0a4bfe4b",
     "grade": false,
     "grade_id": "cell-9573d55a6cc7a79a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Collect Chinese Vocabs\n",
    "all_chinese_vocabs = set()\n",
    "for chinese in lines.chin:\n",
    "    words = chinese.split()\n",
    "    for word in words:\n",
    "        if word not in all_chinese_vocabs:\n",
    "            all_chinese_vocabs.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AqQxd93At8Y_",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aaa117a1a9ef6101a6dcbb4e172ff64c",
     "grade": false,
     "grade_id": "cell-2b285bbaaf0bb29c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's implement the following codes to find the maximum sequence length of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ApMcilUdw6MX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8406af3f5b0446b0750be37157760ddf",
     "grade": false,
     "grade_id": "cell-54947cadd4bdeb58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Max Length of input sequence\n",
    "sequence_length = []\n",
    "for line in lines.eng:\n",
    "    sequence_length.append(len(line.split(' ')))\n",
    "max_length_inp = np.max(sequence_length)\n",
    "print(max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5r8H-xriw6MX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4f46c3196a78a0d7abba9159251659",
     "grade": false,
     "grade_id": "cell-389126c8d63c4124",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Max Length of target sequence\n",
    "sequence_length = []\n",
    "for line in lines.chin:\n",
    "    sequence_length.append(len(line.split(' ')))\n",
    "max_length_targ = np.max(sequence_length)\n",
    "max_length_targ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "z2ID7e3a0-Gw",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94d6bac02b2788a4fe011e9d500c21a4",
     "grade": false,
     "grade_id": "cell-8f6c8ab252764056",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With this, we can see that the maximum input sequence is 8 and the maximum target sequence is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "scApqmY00-Gw",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40d5094d209e6b7975b5e3eca73db2a9",
     "grade": false,
     "grade_id": "cell-05bfb2e9b479f1e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3\n",
    "<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n",
    "#### Task 1\n",
    "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>\n",
    "Sort and store the tokenized English and Chinese words on the variables `input_words` and `target_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "mTOotq_Iw6MY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9078779aa366ba1983eafb52d994367c",
     "grade": false,
     "grade_id": "cell-a233cc05cff264ec",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-3-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-3-Task-1\n",
    "\n",
    "# input_words = None\n",
    "# target_words = None\n",
    "\n",
    "# Sorting and Storing the tokens of English and Chinese words\n",
    "\n",
    "# Exercise 3\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e9MTr3c-0-Gx",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dc42d6bc240ed9b4598a698272bdbae",
     "grade": true,
     "grade_id": "cell-15b8c56f73279370",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-3-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "assert input_words is not None\n",
    "assert target_words is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlODcVXyAo98"
   },
   "source": [
    "#### Task 2\n",
    "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>\n",
    "Since, we are performing Machine translation, we have an encoder and decoder kind of architecture. We will have the encoder architecture as following:\n",
    "\n",
    "<div align=\"center\">\n",
    "<figure>\n",
    "<img src=\"https://doc.google.com/a/fusemachines.com/uc?id=1voHxN0hllGSLfyPJSy6tI_hzTNO6hHRl\" >\n",
    "<figcaption>Figure 1. Machine Translation\n",
    "</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "Here, the green denoted LSTM cells represent the encoder part and the red LSTM cells represent the decoder part of a Machine Translation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ZkXtWETvAnZ5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9db575d6fd33f6657706244f3d78c37",
     "grade": false,
     "grade_id": "cell-d21094fc3564a7c2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "Ex-3-Task-2"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-3-Task-2\n",
    "# counting the total tokens of English and Chinese words\n",
    "num_encoder_tokens = None\n",
    "num_decoder_tokens = None\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION\n",
    "print(num_encoder_tokens, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "idi_diAO0-Gx",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ede6a6c888773ecf37e16e8e132e6643",
     "grade": true,
     "grade_id": "cell-cd48ce4fc6942143",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-3-Task-2"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HZSC0tsow6MY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be0bb5f1bd433ae49ce0865a36b5d4a3",
     "grade": false,
     "grade_id": "cell-040c48796e20c48e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# For zero padding we add one extra token\n",
    "num_decoder_tokens += 1\n",
    "num_encoder_tokens += 1\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MYPMZqntw6MZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f9b24c88907af9d6fc4abfd0f2d3b26",
     "grade": false,
     "grade_id": "cell-df7be80213612242",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# compute and store the tokens with index in dictionary as word, index format\n",
    "input_token_index = dict([(word, i + 1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i + 1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ofUESFVWw6MZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1c30203bafa9868d0d233747ebf40a",
     "grade": false,
     "grade_id": "cell-85cd917cd9596b95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# compute and store the tokens with index in dictionary as index, word format\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "A-jl8mSpw6MZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f4b730ef6492fbd05ede8bffededdea",
     "grade": false,
     "grade_id": "cell-75e847bed23fd5e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# shuffling the lines to make better predictions\n",
    "lines = shuffle(lines)\n",
    "lines.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ciFxeS0l0-Gy",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e855f52ea26ed6a78b5271614a523804",
     "grade": false,
     "grade_id": "cell-4874858156792ac3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "U8-wDveew6Ma",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7eedfd692c48276f98708a8a8409bcd",
     "grade": false,
     "grade_id": "cell-711ad7496e8d5a8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train - Test Split\n",
    "X, y = lines.eng, lines.chin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zSVXqHNaLirV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7857b81bc963cfe85ff7a67874257c74",
     "grade": false,
     "grade_id": "cell-9456558d54c0d0ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Following code is to generate batch of training and testing data. If you are interested in the code you can go line by line and explore the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kM-5q9lbw6Ma",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d776a860adbff380724704f73503def3",
     "grade": false,
     "grade_id": "cell-b1894cc022235615",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    '''Function to generate a batch of data '''\n",
    "    for j in range(0, len(X), batch_size):\n",
    "        encoder_input_data = np.zeros((max_length_inp, batch_size),dtype='float32')\n",
    "        decoder_input_data = np.zeros((max_length_targ, batch_size),dtype='float32')\n",
    "        decoder_target_data = np.zeros((max_length_targ, batch_size ,num_decoder_tokens),dtype='float32')\n",
    "        for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "            for t, word in enumerate(input_text.split()):\n",
    "                encoder_input_data[t, i] = input_token_index[word] # encoder input seq\n",
    "            for t, word in enumerate(target_text.split()):\n",
    "                if t<len(target_text.split())-1:\n",
    "                    decoder_input_data[t, i] = target_token_index[word] # decoder input seq\n",
    "                if t>0:\n",
    "                    # decoder target sequence (one hot encoded)\n",
    "                    # does not include the START_ token\n",
    "                    # Offset by one timestep\n",
    "                    decoder_target_data[t-1, i , target_token_index[word]] = 1.\n",
    "        yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VrCmUOPX0dsn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "108ad531aae4affa988fcf9dbb1216f5",
     "grade": false,
     "grade_id": "cell-5ad4b7a3cc4da9e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Input to the Encoder\n",
    "encoder_input_data = np.zeros((len(lines.eng), 9),dtype='float32')\n",
    "\n",
    "# output from the encoder or input to the decoder \n",
    "decoder_input_data = np.zeros((len(lines.chin), 5),dtype='float32')\n",
    "\n",
    "# output by the decoder\n",
    "decoder_target_data = np.zeros((len(lines.chin), 5, num_decoder_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "z3QkOfY70hQK",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dd8a7bcec06c4a8c969cf1ef6c956b0",
     "grade": false,
     "grade_id": "cell-3c9e2cbc956e1e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.chin)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:\n",
    "            # decoder target data is ahead of decoder input by one timestep\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "aknXbSOLw6Mb",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1acb35327047493c41527dbd89cd048",
     "grade": false,
     "grade_id": "cell-dbc8da1b20dbba17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Encoder - Decoder Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7Wd_xqIbw6Mb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eab08df97a955b75b73b334158d24f58",
     "grade": false,
     "grade_id": "cell-6e9227c55dc44051",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BSPvU8Bn0-G0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d20248101d130edfd111600d207cd56",
     "grade": false,
     "grade_id": "cell-906a013f79181247",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4\n",
    "<b><div style=\"text-align: right\">[POINTS: 4]</div></b>\n",
    "#### Task 1\n",
    "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>\n",
    "Store the hidden state and context vector as a result of encoder outputs on variable `encoder_states`.\n",
    "\n",
    "Store in the form of [__hiddenstate__, __contextstate__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "i5U5W2T9f7Qk",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63deee770ecdaabe4190ed7e5abe4859",
     "grade": false,
     "grade_id": "cell-5375e524cb9be57b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "Ex-4-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-4-Task-1\n",
    "# encoder_states = None\n",
    "\n",
    "# Encoder Architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        return encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sk67U0fRx212",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2aa9b3af095f8429833dca9530395f81",
     "grade": true,
     "grade_id": "cell-71c476202d589f78",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "Ex-4-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "# Intentionally left blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-miqEPX4f7Ql",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78b937a268a0f394a91de96f76b3a5e9",
     "grade": false,
     "grade_id": "cell-ddac4c4be3f9b645",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(num_encoder_tokens, latent_dim, latent_dim)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jwgWEUvqf7Qm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3763687430eae58b6cbbcb7e65d2c712",
     "grade": false,
     "grade_id": "cell-57a28a9db81414dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Size of the one hot vectors that will be the input to the decoder\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Output size of the word embedding NN\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Size of the one hot vectors that will be the output of the decoder\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, enc_states):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.embedding(x)\n",
    "\n",
    "        # (passing encoder's hs, cs - context vectors)\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding, enc_states)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        decoder_states = (hidden_state, cell_state)\n",
    "\n",
    "        return predictions, decoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zuqq79h6f7Qm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f14fa8d3ff8a10b6287be85df6c77281",
     "grade": false,
     "grade_id": "cell-5f7f0bed03f209a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(num_decoder_tokens, latent_dim, latent_dim, num_decoder_tokens)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ycS6r5fkf7Qm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "212805f25732ea0776a6ce3bd0a57458",
     "grade": false,
     "grade_id": "cell-c8fc9c2ef2ac84be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder_LSTM = Encoder_LSTM\n",
    "        self.Decoder_LSTM = Decoder_LSTM\n",
    "\n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = num_decoder_tokens\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "\n",
    "        hidden_state, cell_state = self.Encoder_LSTM(source)\n",
    "\n",
    "        x = target[0]\n",
    "\n",
    "        for i in range(1, target_len):\n",
    "            output, ( hidden_state, cell_state ) = self.Decoder_LSTM(x, (hidden_state, cell_state))\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1) # 1st dimension is word embedding, 0th dimension is batchsize\n",
    "            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KxU_HH2mf7Qm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c324a3bc939ea45333ebc3d6435cbf34",
     "grade": false,
     "grade_id": "cell-61d8cb7f520b0326",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "step = 0\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xu-3m5qg0-G1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07abc23b4cedabba832b7e15d98d3fef",
     "grade": false,
     "grade_id": "cell-cdd4235d80801b39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 2\n",
    "<b><div style=\"text-align: right\">[POINTS: 3]</div></b>\n",
    "Increase the number of epochs and train the model to obtain a good accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cymSsIYQw6Me",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38a67eb038486a9db3fc03603d8b6ed6",
     "grade": false,
     "grade_id": "cell-d5ca1a01a0838cda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Some model hyperparameters\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 256\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZWX1460-zVv6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1506343e4b40cdd6ca2faf2b9c5c3923",
     "grade": false,
     "grade_id": "cell-9fdc16b573193e9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epoch_loss = 0.0\n",
    "best_loss = 999999\n",
    "losses = []\n",
    "best_epoch = -1\n",
    "ts1  = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_list = []\n",
    "    print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "    \n",
    "\n",
    "    model.train(True)\n",
    "    for batch_idx, ( input_data, target_data ) in enumerate(generate_batch(batch_size=batch_size)):\n",
    "        input_data_enc = torch.tensor(input_data[0]).long()\n",
    "        input_data_dec = torch.tensor(input_data[1]).long()\n",
    "        target = torch.tensor(target_data.argmax(2)).long()\n",
    "        \n",
    "        # Pass the input and target for model's forward method\n",
    "        output = model(input_data_enc, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights values using the gradients we calculated using bp \n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss_list.append(loss.item())\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_epoch = epoch\n",
    "        if ((epoch - best_epoch) >= 10):\n",
    "            print(\"no improvement in 10 epochs, break\")\n",
    "            break\n",
    "        print(\"Iterations / loss -  {} / {}\".format(batch_idx,loss.item()))\n",
    "        print()\n",
    "    losses.append(np.mean(epoch_loss_list))\n",
    "\n",
    "torch.save({\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'loss': losses\n",
    "          },\"lstm_seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "K1kgspYr0-G1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a606c7517aefd327f82ecdc8c1e0c180",
     "grade": false,
     "grade_id": "cell-404e9c0d68a46c7c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-4-Task-2"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-4-Task-2\n",
    "loss = None\n",
    "\n",
    "# Model Loss\n",
    "# Store the model's loss from trained above\n",
    "\n",
    "# Exercise 4 | Task 2\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AUJwO-z2LORt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe4b6e1a7da659d7d7bb61358f49fe84",
     "grade": false,
     "grade_id": "cell-cd972e8c828f74d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rcMS1UgA0-G1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f502d775a57040c58d4ee2ec8f3023d",
     "grade": true,
     "grade_id": "cell-2c02ab4a9929d13a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-4-Task-2"
    ]
   },
   "outputs": [],
   "source": [
    "#INTENTIONALLY LEFT BLANK\n",
    "assert loss is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "h50t4Y7f0-G1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d560f9d70599bdd447567969c6bb2f08",
     "grade": false,
     "grade_id": "cell-8989e398a4400b6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, after we performed some preprocessing and model training steps, then we will start working on the model inferencing. We will see how well the model predicts the results. Also, we will discover what can be the possible solution to this problem.\n",
    "\n",
    "If we look at the model training results, we can see that the model is not performing really well. This may be because the LSTM network we are using is not able to learn the appropriate feature inputs. The no. of tokens is also pretty large which is giving the model a hard time to learn the input feature itself. So, the possible solution to these problems could be the `Attention Mechanisms`. We have not used attention mechanism, however if we use attention mechanism the result will surely turn out better.\n",
    "\n",
    "Moreover, we can validate the performance of the model by also inferencing on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vm-p0L85w6Mi",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c8a39c7744b369949a86787e8834f7f",
     "grade": false,
     "grade_id": "cell-c942df3936e27c26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  Decode Sample Sequences\n",
    "\n",
    "Following is the code to decode the input sequence to the machine translation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SEf9I1brw6Mi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a0b45c69b73e5f81bf61c8626dadf94",
     "grade": false,
     "grade_id": "cell-5ed91b4ae4003424",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(Encoder(num_encoder_tokens, latent_dim, latent_dim), Decoder(num_decoder_tokens, latent_dim, latent_dim, num_decoder_tokens))\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"lstm_seq2seq\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def decode_sequence(sentence, max_length=50):\n",
    "    model.eval()\n",
    "    # lower, removing punctuations, \n",
    "    tokens =  (''.join(char for char in re.sub(\" +\", \" \", re.sub(\"'\", '', sentence).lower()) if char not in sets_of_punctuations)).split()\n",
    "\n",
    "    text_to_indices = [ input_token_index[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
    "\n",
    "    outputs = [target_token_index[\"<START>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, ( hidden, cell ) = model.Decoder_LSTM(previous_word, (hidden, cell))\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if best_guess == \"<END>\":\n",
    "            break\n",
    "\n",
    "    translated_sentence = [reverse_target_char_index.get(idx, '<PAD>') for idx in outputs]\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nlUCxJsnw6Mj",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "763c22c34ded6325c5ad21a2a9355592",
     "grade": false,
     "grade_id": "cell-c83a2b40557db068",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluation on Train Dataset\n",
    "\n",
    "Generating the sample to check some of the results predicted by the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "C_pmnLSGw6Mk",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62ea65b73b7feb701332ebb26bccf2e2",
     "grade": false,
     "grade_id": "cell-5ecd5fc414a45548",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "decoded_sentence = decode_sequence(X_train[k:k+1].values[0])\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Chinese Translation:', y_train[k:k+1].values[0])\n",
    "print('Predicted Chinese Translation:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t9-tBDjSw6Mk",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cb0841aa20be247dc2cfa65c9ff8f81",
     "grade": false,
     "grade_id": "cell-ed8c1f20e44cf9ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k+=1\n",
    "decoded_sentence = decode_sequence(X_train[k:k+1].values[0])\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Chinese Translation:', y_train[k:k+1].values[0])\n",
    "print('Predicted Chinese Translation:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3TZxb_9Yw6Ml",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5fa51f0d95647f9eabb3536d3f3ffc8",
     "grade": false,
     "grade_id": "cell-415bd9a5589e425e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k+=1\n",
    "decoded_sentence = decode_sequence(X_train[k:k+1].values[0])\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Chinese Translation:', y_train[k:k+1].values[0])\n",
    "print('Predicted Chinese Translation:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "udM3JsI9w6Ml",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e0a03d70e00916d5e4b79b1f091cb4e",
     "grade": false,
     "grade_id": "cell-7fcb2d3381bba963",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k+=1\n",
    "decoded_sentence = decode_sequence(X_train[k:k+1].values[0])\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Chinese Translation:', y_train[k:k+1].values[0])\n",
    "print('Predicted Chinese Translation:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dCL7Qdt5w6Mm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f36cbe941bac7f99dbc50e5e80d1c331",
     "grade": false,
     "grade_id": "cell-5e36305a19daeb11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k+=1\n",
    "decoded_sentence = decode_sequence(X_train[k:k+1].values[0])\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Chinese Translation:', y_train[k:k+1].values[0])\n",
    "print('Predicted Chinese Translation:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eGmUTVPS0-G3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4482e01ff125db80d9d6e06049834443",
     "grade": false,
     "grade_id": "cell-b60c247b0c3f991f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "CONGRATULATIONS!!! on completing the Assignment."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
